# ğŸ™ï¸ Faster-Whisper Realtime STT - Ãœcretsiz Alternatif

100% Ã¼cretsiz, GPU destekli, gerÃ§ek zamanlÄ± speech-to-text uygulamasÄ±. OpenAI'Ä±n Whisper modelini kullanÄ±r.

## ğŸ¯ Neden Bu Uygulama?

| Ã–zellik | ElevenLabs Scribe | Faster-Whisper (Bu Uygulama) |
|---------|-------------------|------------------------------|
| **Maliyet** | ~$0.10/dakika | **%100 ÃœCRETSIZ** |
| **Latency** | ~150ms | ~1-2 saniye (GPU) |
| **TÃ¼rkÃ§e Kalitesi** | MÃ¼kemmel | MÃ¼kemmel (large-v3) |
| **GPU Gereksinimi** | Yok | Var (Colab'da Ã¼cretsiz) |
| **Dil DesteÄŸi** | 8 dil | 99+ dil |
| **Offline KullanÄ±m** | HayÄ±r | Evet |

## âœ¨ Ã–zellikler

- ğŸ†“ **%100 Ãœcretsiz**: Google Colab'Ä±n Ã¼cretsiz GPU'sunu kullanÄ±r
- ğŸš€ **GerÃ§ek ZamanlÄ±**: ~1-2 saniye latency (GPU ile)
- ğŸŒ **99+ Dil DesteÄŸi**: TÃ¼rkÃ§e, Ä°ngilizce ve daha fazlasÄ±
- ğŸšï¸ **Model SeÃ§enekleri**: tiny (75MB) â†’ large-v3 (3GB)
- ğŸ™ï¸ **AkÄ±llÄ± VAD (Silero)**: Sessizlik algÄ±landÄ±ÄŸÄ±nda otomatik iÅŸler - sabit dÃ¶ngÃ¼ yok!
- ğŸ“Š **Latency Monitoring**: GerÃ§ek zamanlÄ± performans takibi
- ğŸ¨ **Modern UI**: Responsive ve kullanÄ±cÄ± dostu arayÃ¼z (en yeni log en Ã¼stte)
- â˜ï¸ **Colab Ready**: Google Colab'da Ã§alÄ±ÅŸmaya hazÄ±r

## ğŸ—ï¸ Proje YapÄ±sÄ±

```
whisperRealTime/
â”œâ”€â”€ app.py                # Flask backend (WebSocket)
â”œâ”€â”€ colab_setup.py       # Colab baÅŸlatma script'i
â”œâ”€â”€ requirements.txt     # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html      # Ana web arayÃ¼zÃ¼
â””â”€â”€ static/
    â””â”€â”€ js/
        â””â”€â”€ (embedded in index.html)
```

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§ - Google Colab (Ã–nerilen)

### 1ï¸âƒ£ Colab Notebook OluÅŸtur

Yeni bir Google Colab notebook aÃ§Ä±n: https://colab.research.google.com

### 2ï¸âƒ£ GPU'yu EtkinleÅŸtir

```
MenÃ¼ â†’ Runtime â†’ Change runtime type â†’ GPU â†’ Save
```

### 3ï¸âƒ£ DosyalarÄ± YÃ¼kle

**SeÃ§enek A: GitHub'dan (Ã–nerilen)**
```python
!git clone https://github.com/your-username/your-repo.git
%cd your-repo/whisperRealTime
```

**SeÃ§enek B: Manuel Upload**
- Sol panel â†’ Files â†’ Upload
- TÃ¼m dosyalarÄ± yÃ¼kleyin

### 4ï¸âƒ£ UygulamayÄ± BaÅŸlat

```python
# Basit baÅŸlatma
from colab_setup import main
main()
```

**Veya model boyutu seÃ§erek:**

```python
# Tiny model (en hÄ±zlÄ±, CPU iÃ§in)
from colab_setup import quick_start_cpu
quick_start_cpu()

# Small model (dengeli, Ã¶nerilen)
from colab_setup import quick_start_gpu
quick_start_gpu()

# Large-v3 (en kaliteli, GPU gerekli)
from colab_setup import quick_start_large
quick_start_large()
```

### 5ï¸âƒ£ Kullan!

- Console'da gÃ¶rÃ¼nen **ngrok URL**'ini kopyalayÄ±n
- TarayÄ±cÄ±nÄ±zda aÃ§Ä±n
- Mikrofon iznini verin
- "BaÅŸlat" butonuna tÄ±klayÄ±n
- KonuÅŸmaya baÅŸlayÄ±n! ğŸ¤

## ğŸ’» Yerel Ortamda KullanÄ±m

### Gereksinimler

- Python 3.8+
- (Opsiyonel) NVIDIA GPU + CUDA

### Kurulum

```bash
# 1. BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin
pip install -r requirements.txt

# 2. UygulamayÄ± baÅŸlatÄ±n
python app.py
```

### Model ve GPU AyarlarÄ±

```bash
# Model seÃ§imi (varsayÄ±lan: small)
export WHISPER_MODEL=small  # tiny, base, small, medium, large-v3

# GPU kullanÄ±mÄ± (varsayÄ±lan: 1 - aktif)
export USE_GPU=1  # 1: GPU kullan, 0: CPU kullan

# BaÅŸlat
python app.py
```

TarayÄ±cÄ±nÄ±zda aÃ§Ä±n: `http://localhost:5123`

## âš™ï¸ Model SeÃ§imi

| Model | Boyut | HÄ±z | Kalite | Ã–nerilen |
|-------|-------|-----|--------|----------|
| `tiny` | ~75MB | âš¡âš¡âš¡âš¡âš¡ | â­â­ | CPU iÃ§in |
| `base` | ~150MB | âš¡âš¡âš¡âš¡ | â­â­â­ | CPU iÃ§in |
| `small` | ~500MB | âš¡âš¡âš¡ | â­â­â­â­ | **Dengeli (GPU)** |
| `medium` | ~1.5GB | âš¡âš¡ | â­â­â­â­â­ | GPU iÃ§in |
| `large-v3` | ~3GB | âš¡ | â­â­â­â­â­â­ | **En Kaliteli (GPU)** |

### Model SeÃ§im Tavsiyeleri

**Google Colab (Ãœcretsiz GPU):**
- `small`: HÄ±z ve kalite dengesi (Ã¶nerilen)
- `large-v3`: En iyi TÃ¼rkÃ§e kalitesi

**Yerel CPU:**
- `tiny`: En hÄ±zlÄ±
- `base`: Daha iyi kalite

**Yerel GPU (NVIDIA):**
- `small` veya `medium`: HÄ±zlÄ± ve kaliteli
- `large-v3`: En iyi sonuÃ§

## ğŸ”§ KonfigÃ¼rasyon

### Dil AyarlarÄ±

```javascript
// Frontend'de (index.html)
languageSelect: 'tr'  // tr, en, de, fr, es, pt, it, ar, zh, ja, auto
```

### Audio AyarlarÄ±

```javascript
sampleRate: 16000      // 8000, 16000 (Ã¶nerilen), 22050, 44100, 48000
chunkLength: 3         // Her kaÃ§ saniyede bir iÅŸlensin (1-10)
vadFilter: true        // Sessizlik filtreleme (true Ã¶nerilen)
```

### Backend AyarlarÄ± (Environment Variables)

```bash
WHISPER_MODEL=small    # Model boyutu
USE_GPU=1             # GPU kullanÄ±mÄ± (1: evet, 0: hayÄ±r)
```

## ğŸ“Š WebSocket ProtokolÃ¼

### Client â†’ Server

**Config Update:**
```json
{
  "type": "config",
  "config": {
    "language": "tr",
    "chunk_length_s": 3,
    "vad_filter": true
  }
}
```

**Audio Chunk:**
```json
{
  "type": "audio",
  "audio_base_64": "base64_encoded_pcm_data",
  "sample_rate": 16000
}
```

### Server â†’ Client

**Session Started:**
```json
{
  "type": "session_started",
  "config": {...}
}
```

**Partial Update (Buffer Status):**
```json
{
  "type": "partial_transcript",
  "text": "[3.2s audio buffered...]",
  "buffer_duration": 3.2
}
```

**Committed Transcript:**
```json
{
  "type": "committed_transcript",
  "text": "transkripsiyon metni",
  "language_code": "tr",
  "latency_ms": 1234,
  "words": [...],
  "buffer_duration": 3.0
}
```

## ğŸ”Œ API Endpoint'leri

### `GET /`
Ana web arayÃ¼zÃ¼

### `GET /health`
Sistem durumu ve model bilgisi

**Response:**
```json
{
  "status": "ok",
  "model": "small",
  "gpu": true,
  "gpu_available": true,
  "gpu_name": "Tesla T4",
  "model_loaded": true
}
```

### `GET /config`
Model ve dil konfigÃ¼rasyonu

### `WS /ws`
WebSocket endpoint (realtime transcription)

## ğŸ› Troubleshooting

### "faster-whisper yÃ¼klÃ¼ deÄŸil" HatasÄ±

```bash
pip install faster-whisper
```

### "CUDA bulunamadÄ±" UyarÄ±sÄ±

- **Colab'da:** Runtime â†’ Change runtime type â†’ GPU â†’ Save
- **Yerel:** CUDA Toolkit kurun veya `USE_GPU=0` ile CPU kullanÄ±n

### Mikrofon Ã‡alÄ±ÅŸmÄ±yor

- TarayÄ±cÄ± izinlerini kontrol edin
- HTTPS baÄŸlantÄ±sÄ± gerekli (ngrok otomatik saÄŸlar)
- Chrome/Edge kullanmanÄ±z Ã¶nerilir

### ngrok Token Gerekiyor

```python
# Colab'da
from pyngrok import ngrok
ngrok.set_auth_token("your-token-here")
```

Token almak iÃ§in: https://dashboard.ngrok.com/get-started/your-authtoken

### Model Ä°ndirme Ã‡ok YavaÅŸ

- Ä°lk kullanÄ±mda model otomatik indirilir
- `small` model ~500MB (2-3 dakika)
- `large-v3` model ~3GB (5-10 dakika)
- Model bir kez indirilir, sonra cache'ten kullanÄ±lÄ±r

### YÃ¼ksek Latency (>5 saniye)

**Colab'da:**
- GPU'yu etkinleÅŸtirin (Runtime â†’ Change runtime type â†’ GPU)
- Daha kÃ¼Ã§Ã¼k model deneyin (`small` yerine `tiny`)

**Yerel CPU'da:**
- Normal (CPU'da 3-5 saniye)
- `tiny` veya `base` model kullanÄ±n

## ğŸ“ˆ Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±

### Latency Testleri

| Ortam | Model | Latency | Kalite |
|-------|-------|---------|--------|
| Colab GPU (T4) | tiny | ~500ms | Orta |
| Colab GPU (T4) | small | ~1-2s | Ä°yi |
| Colab GPU (T4) | large-v3 | ~2-3s | MÃ¼kemmel |
| Colab CPU | tiny | ~2-3s | Orta |
| Local CPU (i7) | tiny | ~3-5s | Orta |
| Local GPU (RTX 3060) | small | ~800ms | Ä°yi |

### Ä°puÃ§larÄ±

1. **En DÃ¼ÅŸÃ¼k Latency Ä°Ã§in:**
   - GPU kullanÄ±n
   - `tiny` veya `small` model
   - `chunk_length_s: 2`

2. **En YÃ¼ksek Kalite Ä°Ã§in:**
   - GPU kullanÄ±n
   - `large-v3` model
   - VAD filter aktif
   - `chunk_length_s: 3-5`

3. **Dengeli (Ã–nerilen):**
   - Colab GPU
   - `small` model
   - `chunk_length_s: 3`
   - VAD aktif

## ğŸ†š ElevenLabs ile KarÅŸÄ±laÅŸtÄ±rma

### Faster-Whisper AvantajlarÄ± âœ…

- âœ… %100 Ã¼cretsiz (Colab)
- âœ… Offline Ã§alÄ±ÅŸabilir
- âœ… 99+ dil desteÄŸi
- âœ… Veri gizliliÄŸi (kendi sunucunuz)
- âœ… SÄ±nÄ±rsÄ±z kullanÄ±m

### ElevenLabs AvantajlarÄ± âœ…

- âœ… Ã‡ok dÃ¼ÅŸÃ¼k latency (~150ms)
- âœ… Setup gerektirmez
- âœ… GPU gerekmez
- âœ… Bulut tabanlÄ± (her yerden eriÅŸim)

### Hangi Durumda Ne KullanmalÄ±?

**Faster-Whisper (Bu Uygulama) Kullan:**
- YÃ¼ksek kullanÄ±m hacmi (>100 dakika/gÃ¼n)
- BÃ¼tÃ§e kÄ±sÄ±tlÄ±
- Veri gizliliÄŸi Ã¶nemli
- Offline Ã§alÄ±ÅŸma gerekli
- Test ve development

**ElevenLabs Kullan:**
- Ultra-dÃ¼ÅŸÃ¼k latency kritik (<200ms)
- Production kullanÄ±m (gÃ¼venilirlik)
- DÃ¼ÅŸÃ¼k kullanÄ±m hacmi (<30 dakika/gÃ¼n)
- Setup yapmak istemiyorsanÄ±z

## ğŸ¤ KatkÄ±da Bulunma

Pull request'ler memnuniyetle karÅŸÄ±lanÄ±r!

1. Fork edin
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. Commit edin (`git commit -m 'feat: add amazing feature'`)
4. Push edin (`git push origin feature/amazing-feature`)
5. Pull Request aÃ§Ä±n

## ğŸ“ Lisans

Bu proje test ve eÄŸitim amaÃ§lÄ±dÄ±r. OpenAI'Ä±n Whisper modeli MIT lisansÄ± altÄ±nda sunulmaktadÄ±r.

## ğŸ”— FaydalÄ± Linkler

- [Faster-Whisper GitHub](https://github.com/guillaumekln/faster-whisper)
- [OpenAI Whisper](https://github.com/openai/whisper)
- [Google Colab](https://colab.research.google.com)
- [ngrok Documentation](https://ngrok.com/docs)
- [Flask-Sock Documentation](https://flask-sock.readthedocs.io/)

## ğŸ“§ Ä°letiÅŸim

SorularÄ±nÄ±z iÃ§in issue aÃ§abilirsiniz.

---

**Not:** Bu uygulama OpenAI'Ä±n Whisper modelini kullanÄ±r ve Google Colab'Ä±n Ã¼cretsiz GPU'sundan yararlanÄ±r. Colab kullanÄ±m politikalarÄ±na uygun ÅŸekilde kullanÄ±n.

## ğŸ“ Ek Bilgiler

### Model DetaylarÄ±

Faster-Whisper, OpenAI Whisper'Ä±n [CTranslate2](https://github.com/OpenNMT/CTranslate2) ile optimize edilmiÅŸ versiyonudur:
- 4x daha hÄ±zlÄ±
- 2x daha az bellek
- AynÄ± kalite

### Desteklenen Diller

TÃ¼rkÃ§e, Ä°ngilizce, Almanca, FransÄ±zca, Ä°spanyolca, Portekizce, Ä°talyanca, ArapÃ§a, Ã‡ince, Japonca, Korece, RusÃ§a, HintÃ§e ve 80+ dil daha.

Tam liste: https://github.com/openai/whisper#available-models-and-languages

### GPU Gereksinimleri

| Model | VRAM | GPU Ã–nerisi |
|-------|------|-------------|
| tiny | ~1GB | Herhangi bir GPU |
| base | ~1GB | Herhangi bir GPU |
| small | ~2GB | GTX 1060+, Colab T4 |
| medium | ~5GB | RTX 2060+, Colab T4 |
| large-v3 | ~10GB | RTX 3080+, A100 |

Google Colab Ã¼cretsiz T4 GPU: 16GB VRAM (tÃ¼m modeller Ã§alÄ±ÅŸÄ±r)

### Colab SÄ±nÄ±rlamalarÄ±

- **Ãœcretsiz Tier**: ~12 saat session, sonra restart
- **RAM**: 12GB (yeterli)
- **Disk**: 100GB (yeterli)
- **GPU**: Tesla T4 (16GB VRAM, mÃ¼kemmel)

Session disconnect olursa setup script'i tekrar Ã§alÄ±ÅŸtÄ±rmanÄ±z yeterli.
